{"cells":[{"metadata":{"_uuid":"90e3a2685594cc6aae3914d3d519be153b050d78"},"cell_type":"markdown","source":"<center>\n<img src=\"https://habrastorage.org/files/fd4/502/43d/fd450243dd604b81b9713213a247aa20.jpg\" />\n</center> \n     \n## <center>  [mlcourse.ai](https://mlcourse.ai) â€“ Open Machine Learning Course \n\n#### <center> Author: [Yury Kashnitskiy](https://yorko.github.io) (@yorko) \n\n# <center>Assignment #3. Spring 2019\n## <center> Part 3. Gradient boosting"},{"metadata":{"_uuid":"1cc3855272f09bf4b32a22bcf317d7ea22564706"},"cell_type":"markdown","source":"**In this assignment, you're asked to beat a baseline in the [\"Flight delays\" competition](https://www.kaggle.com/c/flight-delays-fall-2018).**\n\nPrior to working on the assignment, you'd better check out the corresponding course material:\n 1. [Classification, Decision Trees and k Nearest Neighbors](https://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic03_decision_trees_kNN/topic3_decision_trees_kNN.ipynb?flush_cache=true), the same as an interactive web-based [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-3-decision-trees-and-knn) \n 2. Ensembles:\n  - [Bagging](https://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic05_ensembles_random_forests/topic5_part1_bagging.ipynb?flush_cache=true), the same as a [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-1-bagging)\n  - [Random Forest](https://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic05_ensembles_random_forests/topic5_part2_random_forest.ipynb?flush_cache=true), the same as a [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-2-random-forest)\n  - [Feature Importance](https://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic05_ensembles_random_forests/topic5_part3_feature_importance.ipynb?flush_cache=true), the same as a [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-3-feature-importance)\n 3. - [Gradient boosting](https://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic10_boosting/topic10_gradient_boosting.ipynb?flush_cache=true), the same as a [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-10-gradient-boosting) \n   - Logistic regression, Random Forest, and LightGBM in the \"Kaggle Forest Cover Type Prediction\" competition: [Kernel](https://www.kaggle.com/kashnitsky/topic-10-practice-with-logit-rf-and-lightgbm) \n 4. You can also practice with demo assignments, which are simpler and already shared with solutions:\n  - \"Decision trees with a toy task and the UCI Adult dataset\": [assignment](https://www.kaggle.com/kashnitsky/a3-demo-decision-trees) + [solution](https://www.kaggle.com/kashnitsky/a3-demo-decision-trees-solution)\n  - \"Logistic Regression and Random Forest in the credit scoring problem\": [assignment](https://www.kaggle.com/kashnitsky/assignment-5-logit-and-rf-for-credit-scoring) + [solution](https://www.kaggle.com/kashnitsky/a5-demo-logit-and-rf-for-credit-scoring-sol)\n\n 5. There are also 7 video lectures on trees, forests, boosting and their applications: [mlcourse.ai/video](https://mlcourse.ai/video) \n\n### Your task is to:\n 1. beat **\"A3 baseline (8 credits)\"** on Public LB (**0.73449** LB score)\n 2. rename your [team](https://www.kaggle.com/c/flight-delays-fall-2018/team) in full accordance with the course rating\n \n This task is intended to be relatively easy. Here you are not required to upload your reproducible solution.\n \n### <center> Deadline for A3: 2019 March 31, 20:59 GMT (London time)"},{"metadata":{"trusted":true,"_uuid":"b486c4beb6f64fbeb70610a0e5ed7145256605e1"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e7833f60e9392ba0526aec5b68ea8587ce90274"},"cell_type":"code","source":"train_df = pd.read_csv('../input/flight_delays_train.csv')\ntest_df = pd.read_csv('../input/flight_delays_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c02aa9ea53a2733cd5683bd10beed2b561acf7bb"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a0f1e18cc9b10cfd4f098183511f21722e61752"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce71f38fec3d94108609ef0cae861010ab0e8ca6"},"cell_type":"markdown","source":"Given flight departure time, carrier's code, departure airport, destination location, and flight distance, you have to predict departure delay for more than 15 minutes. As the simplest benchmark, let's take logistic regression and two features that are easiest to take: DepTime and Distance. This will correspond to **\"simple logit baseline\"** on Public LB."},{"metadata":{"trusted":true,"_uuid":"5bac98797b2bfb23851d735ec3761fc0e1401e61"},"cell_type":"code","source":"X_train, y_train = train_df[['Distance', 'DepTime']].values, train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values\nX_test = test_df[['Distance', 'DepTime']].values\n\nX_train_part, X_valid, y_train_part, y_valid = \\\n    train_test_split(X_train, y_train, \n                     test_size=0.3, random_state=17)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e99742ad807b66899de68241b60cf8310fb306c"},"cell_type":"code","source":"logit_pipe = Pipeline([('scaler', StandardScaler()),\n                       ('logit', LogisticRegression(C=1, random_state=17, solver='liblinear'))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"309be359915c00cbad41384a08926ddf25d47c7b"},"cell_type":"code","source":"logit_pipe.fit(X_train_part, y_train_part)\nlogit_valid_pred = logit_pipe.predict_proba(X_valid)[:, 1]\n\nroc_auc_score(y_valid, logit_valid_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"082bfede543f62192c55146294cdb23b3b898c90"},"cell_type":"code","source":"logit_pipe.fit(X_train, y_train)\nlogit_test_pred = logit_pipe.predict_proba(X_test)[:, 1]\n\npd.Series(logit_test_pred, \n          name='dep_delayed_15min').to_csv('logit_2feat.csv', \n                                           index_label='id', header=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efd64f65bd3ac9d7fd279513a8cf49e7f2cb7ac0"},"cell_type":"markdown","source":"Now you have to beat **\"A3 baseline (8 credits)\"** on Public LB. It's not challenging at all. Go for LightGBM, maybe some other models (or ensembling) as well. Include categorical features, do some simple feature engineering as well. Good luck!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}